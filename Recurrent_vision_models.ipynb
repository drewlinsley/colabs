{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Recurrent_vision_models",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drewlinsley/colabs/blob/master/Recurrent_vision_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "dgGk4vLIV8RS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**0. DISCoV 2/7/19**\n",
        "You can find the [presentation link here](https://docs.google.com/presentation/d/1pJi1fPt7i7enJAClSVkPOw3pl515YN0Q44kvO8zSpdc/edit?usp=sharing)."
      ]
    },
    {
      "metadata": {
        "id": "bqDrZiMSVfQO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**1. Python imports.**\n",
        "\n",
        "Let's split this into separate blocks for clarity.\n",
        "\n",
        "Here, you will import `drive` from the google colab library for connecting to [google drive](https://github.com/googlecolab/colabtools/blob/master/google/colab/drive.py) and `files` for [uploading local files](https://github.com/googlecolab/colabtools/blob/master/google/colab/files.py) (i.e. your machine) to this kernel. Finally, MediaFileUpload is a class for more efficient [uploads](https://github.com/googleapis/google-api-python-client/blob/master/googleapiclient/http.py). "
      ]
    },
    {
      "metadata": {
        "id": "SHIsvXTP9lT3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "from google.colab import files as cfiles\n",
        "from googleapiclient.http import MediaFileUpload\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BEdxLGEYVkuX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np  # Note numpy is aliased as np\n",
        "from PIL import Image\n",
        "import os\n",
        "import shutil\n",
        "from glob import glob  # File path collection\n",
        "import tensorflow as tf  # Note tensorflow is aliased as tf\n",
        "from matplotlib import pyplot as plt  # Library for plotting images\n",
        "\n",
        "# Keras model utilities\n",
        "from keras.models import Model  # A Keras class for constructing a deep neural network model\n",
        "from keras.models import Sequential  # A Keras class for connecting deep neural network layers \n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        "from keras.preprocessing.image import ImageDataGenerator  # A class for data loading during model training\n",
        "from keras.utils import np_utils\n",
        "\n",
        "# Keras ResNet routines\n",
        "from keras.applications.resnet50 import ResNet50  # Import the ResNet deep neural network\n",
        "from keras.preprocessing import image  # Routines for loading image data\n",
        "from keras.applications.resnet50 import preprocess_input  # ResNet-specific routines for preprocessing images\n",
        "from keras.applications.resnet50 import decode_predictions  # ResNet-specific routines for extracting predictions\n",
        "\n",
        "# Keras layers\n",
        "from keras.layers import Dense  # A fully connected neural networld layer\n",
        "from keras.layers import Activation  # A class for point-wise nonlinearity layers\n",
        "from keras.layers import Flatten  # Reshape a tensor into a matrix\n",
        "from keras.layers import Dropout  # A regularization layer which randomly zeros neural network units during training.\n",
        "from keras.layers import InputLayer\n",
        "from keras.layers import Lambda\n",
        "from keras.layers.pooling import GlobalAveragePooling2D\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import BatchNormalization\n",
        "\n",
        "# Optimizers\n",
        "from keras.optimizers import Adam  # Adam optimizer https://arxiv.org/abs/1412.6980\n",
        "from keras.optimizers import SGD  # Stochastic gradient descent optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0vQJXr_STYXw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will also install the google_images_download library to scrape images from websites. Later, we will pass these images through trained deep neural networks.\n",
        "\n",
        "See how we can install libraries in python using the `pip` command. This is the python package manager, and is typically called with : `pip install my_package`. Note that because we are using the jupyter/ipython interface, we have to prepend an exclamation point `!` to call pip with the command line interpreter.\n",
        "\n",
        "Install keras-contrib to get a resnet18.\n",
        "Clone labmeeting from my repo to get the hgru/fgru modules.\n",
        "Clone foolbox to apply adversarial perturbations to images.\n",
        "\n",
        "Download CIFAR-100 for training models."
      ]
    },
    {
      "metadata": {
        "id": "loerA-vW6XtL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install google_images_download\n",
        "from google_images_download import google_images_download   #importing the library\n",
        "\n",
        "# Clone labmeeting modules\n",
        "!rm -rf labmeeting\n",
        "!git clone https://github.com/drewlinsley/labmeeting\n",
        "!touch labmeeting/__init__.py\n",
        "!ls labmeeting/\n",
        "\n",
        "# Clone foolbox\n",
        "!pip install foolbox\n",
        "\n",
        "# Download CIFAR-100\n",
        "from keras.datasets import cifar100\n",
        "(x_train, y_train), (x_test, y_test) = cifar100.load_data(label_mode='fine')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GMZM6iZQbicB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**2. Set global variables**\n",
        "This is not \"good programming practice\" under typical circumstances, but it is reasonable for python notebooks. So bear with me.\n",
        "\n",
        "We will set global variables (paths, etc.) and mount google drive here."
      ]
    },
    {
      "metadata": {
        "id": "1QXopZ3WcEuc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def make_dir(d):\n",
        "    \"\"\"Make directory d if it does not exist.\"\"\"\n",
        "    if not os.path.exists(d):\n",
        "        os.makedirs(d)\n",
        "\n",
        "\n",
        "IMG_DIR  = \"/content/image_dataset\"\n",
        "CKPT_DIR  = \"/content/image_dataset\"\n",
        "PROC_DIR = \"%s_processed\" % IMG_DIR\n",
        "# drive.mount(\"/content/gdrive\")\n",
        "print(\"TensorFlow version: \" + tf.__version__)\n",
        "\n",
        "make_dir(IMG_DIR)\n",
        "make_dir(PROC_DIR)\n",
        "\n",
        "# # If necessary, clear out the directories\n",
        "# !rm -rf /content/image_dataset\n",
        "# !rm -rf /content/image_dataset_processed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DZoQIbT0NDmL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**3. Augmentations**\n",
        "\n",
        "CNNs are very sample inefficient: they need to be exposed to large amounts of image-level variability to reach their potential in image classification. This is because CNNs have very weak biases about natural images, relying only on convolutions that implement local-weight sharing.\n",
        "\n",
        "Image datasets can be augmented with all sorts of transformations to expose CNNs to more variability and improve performance. You can do this easily in Keras with the built-in  [ImageDataGenerator class](https://keras.io/preprocessing/image/).\n",
        "\n",
        "We set our batch size to 32 (the number of images we process at once during training), since this is a minimal size to use in a model that incorporates so-called \"batch normalization\" (like the ResNet).\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "3pm7nzCVdvbV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 32  # Number of images to process at once\n",
        "height = 32\n",
        "width = 32\n",
        "channels = 3\n",
        "nb_classes = 100\n",
        "input_shape = [height, width, channels]\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "mean = x_train.mean((0, 1, 2))\n",
        "std = x_train.std((0, 1, 2))\n",
        "\n",
        "x_train -= mean\n",
        "x_train /= std\n",
        "x_test = x_test.astype('float32')\n",
        "x_test -= mean\n",
        "x_test /= std\n",
        "y_train = np_utils.to_categorical(y_train, nb_classes)\n",
        "y_test = np_utils.to_categorical(y_test, nb_classes)\n",
        "\n",
        "generator = ImageDataGenerator()\n",
        "    # rotation_range=15,\n",
        "    # width_shift_range=5./width,\n",
        "    # height_shift_range=5./height)\n",
        "generator.fit(x_train, seed=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-utbfatbs8cN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**4. Build the models**"
      ]
    },
    {
      "metadata": {
        "id": "swZuMpVsZosq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "An fGRU + GAP + readout\n",
        "[fgru class](https://github.com/drewlinsley/labmeeting/blob/master/fgru_linear.py)"
      ]
    },
    {
      "metadata": {
        "id": "tm0DiMSoo6iu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from labmeeting import fgru_sigmoid, fgru_linear\n",
        "\n",
        "\n",
        "def fgru_model(input_shape, classes, batch_size, training=True):\n",
        "  \"\"\"Create an htd-fgru model.\"\"\"\n",
        "  def apply_fgru(x):\n",
        "    fgru_module = fgru_linear.hGRU(\n",
        "      'fgru',\n",
        "      x_shape=[batch_size] + input_shape[:-1] + [20],\n",
        "      timesteps=8,\n",
        "      h_ext=[{'h1': [15, 15]}, {'h2': [1, 1]}, {'fb1': [1, 1]}],\n",
        "      strides=[1, 1, 1, 1],\n",
        "      hgru_ids=[{'h1': 20}, {'h2': 128}, {'fb1': 20}],  # Num features per layer\n",
        "      hgru_idx=[{'h1': 0}, {'h2': 1}, {'fb1': 2}],\n",
        "      padding='SAME',\n",
        "      aux={\n",
        "          'readout': 'l2',  # Readout from fgru embedding\n",
        "          'intermediate_ff': [32, 128],\n",
        "          'intermediate_ks': [[3, 3], [3, 3]],\n",
        "          'intermediate_repeats': [3, 3],\n",
        "          'while_loop': False,\n",
        "          'skip': True,\n",
        "          'symmetric_weights': False,\n",
        "          'include_pooling': True\n",
        "      },\n",
        "      pool_strides=[2, 2],\n",
        "      pooling_kernel=[2, 2],\n",
        "      train=training)\n",
        "    return fgru_module.build(x)\n",
        "\n",
        "  # Create model\n",
        "  fgru = Sequential()\n",
        "  fgru.add(Conv2D(kernel_size=(5, 5), padding='SAME', filters=20, input_shape=input_shape))\n",
        "  fgru.add(Lambda(apply_fgru))\n",
        "  fgru.add(BatchNormalization())\n",
        "  fgru.add(GlobalAveragePooling2D())\n",
        "  fgru.add(Dense(nb_classes, activation='softmax'))\n",
        "  return fgru\n",
        "\n",
        "fgru = fgru_model(\n",
        "    input_shape=input_shape,\n",
        "    classes=nb_classes,\n",
        "    batch_size=batch_size,\n",
        "    training=True)\n",
        "print(fgru.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U8KZ9BAwZipr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A conv + hgru + GAP + readout\n",
        "[hgru class](https://github.com/drewlinsley/labmeeting/blob/master/hgru_bn_for.py)"
      ]
    },
    {
      "metadata": {
        "id": "KoJAKQI0o6dj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from labmeeting import hgru_bn_for\n",
        "\n",
        "\n",
        "def hgru_model(input_shape, classes, batch_size, training=True):\n",
        "  \"\"\"Create an htd-fgru model.\"\"\"\n",
        "  def apply_hgru(x):\n",
        "    hgru_module = hgru_bn_for.hGRU(\n",
        "      'hgru_1',\n",
        "      x_shape=[batch_size] + input_shape[:-1] + [20],\n",
        "      timesteps=8,\n",
        "      h_ext=15,\n",
        "      strides=[1, 1, 1, 1],\n",
        "      padding='SAME',\n",
        "      aux={\n",
        "        'reuse': False,\n",
        "        'constrain': False,\n",
        "        'hidden_init': 'zeros',\n",
        "        'symmetric_weights': False},\n",
        "      train=training)\n",
        "    return hgru_module.build(x)\n",
        "  hgru = Sequential()\n",
        "  hgru.add(Conv2D(kernel_size=(5, 5), padding='SAME', filters=20, input_shape=input_shape))\n",
        "  hgru.add(Lambda(apply_hgru))\n",
        "  hgru.add(BatchNormalization())\n",
        "  hgru.add(GlobalAveragePooling2D())\n",
        "  hgru.add(Dense(nb_classes, activation='softmax'))\n",
        "  return hgru\n",
        "\n",
        "hgru = hgru_model(\n",
        "    input_shape=input_shape,\n",
        "    classes=nb_classes,\n",
        "    batch_size=batch_size,\n",
        "    training=True)\n",
        "print(hgru.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Mi-jg00Xd6ha",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Clean and simple Keras implementation of network architectures described in:\n",
        "    - (ResNet-50) [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385.pdf).\n",
        "    - (ResNeXt-50 32x4d) [Aggregated Residual Transformations for Deep Neural Networks](https://arxiv.org/pdf/1611.05431.pdf).\n",
        "    \n",
        "Python 3.\n",
        "\"\"\"\n",
        "\n",
        "from keras import layers\n",
        "from keras import models\n",
        "\n",
        "\n",
        "\n",
        "#\n",
        "# network params\n",
        "#\n",
        "\n",
        "cardinality = 1  # 32 for resnext\n",
        "\n",
        "\n",
        "def residual_network(x, nb_classes):\n",
        "    \"\"\"\n",
        "    ResNeXt by default. For ResNet set `cardinality` = 1 above.\n",
        "    \n",
        "    \"\"\"\n",
        "    def add_common_layers(y):\n",
        "        y = layers.BatchNormalization()(y)\n",
        "        y = layers.LeakyReLU()(y)\n",
        "\n",
        "        return y\n",
        "\n",
        "    def grouped_convolution(y, nb_channels, _strides):\n",
        "        # when `cardinality` == 1 this is just a standard convolution\n",
        "        if cardinality == 1:\n",
        "            return layers.Conv2D(nb_channels, kernel_size=(3, 3), strides=_strides, padding='same')(y)\n",
        "        \n",
        "        assert not nb_channels % cardinality\n",
        "        _d = nb_channels // cardinality\n",
        "\n",
        "        # in a grouped convolution layer, input and output channels are divided into `cardinality` groups,\n",
        "        # and convolutions are separately performed within each group\n",
        "        groups = []\n",
        "        for j in range(cardinality):\n",
        "            group = layers.Lambda(lambda z: z[:, :, :, j * _d:j * _d + _d])(y)\n",
        "            groups.append(layers.Conv2D(_d, kernel_size=(3, 3), strides=_strides, padding='same')(group))\n",
        "            \n",
        "        # the grouped convolutional layer concatenates them as the outputs of the layer\n",
        "        y = layers.concatenate(groups)\n",
        "\n",
        "        return y\n",
        "\n",
        "    def residual_block(y, nb_channels_in, nb_channels_out, _strides=(1, 1), _project_shortcut=False):\n",
        "        \"\"\"\n",
        "        Our network consists of a stack of residual blocks. These blocks have the same topology,\n",
        "        and are subject to two simple rules:\n",
        "\n",
        "        - If producing spatial maps of the same size, the blocks share the same hyper-parameters (width and filter sizes).\n",
        "        - Each time the spatial map is down-sampled by a factor of 2, the width of the blocks is multiplied by a factor of 2.\n",
        "        \"\"\"\n",
        "        shortcut = y\n",
        "\n",
        "        # we modify the residual building block as a bottleneck design to make the network more economical\n",
        "        y = layers.Conv2D(nb_channels_in, kernel_size=(1, 1), strides=(1, 1), padding='same')(y)\n",
        "        y = add_common_layers(y)\n",
        "\n",
        "        # ResNeXt (identical to ResNet when `cardinality` == 1)\n",
        "        y = grouped_convolution(y, nb_channels_in, _strides=_strides)\n",
        "        y = add_common_layers(y)\n",
        "\n",
        "        y = layers.Conv2D(nb_channels_out, kernel_size=(1, 1), strides=(1, 1), padding='same')(y)\n",
        "        # batch normalization is employed after aggregating the transformations and before adding to the shortcut\n",
        "        y = layers.BatchNormalization()(y)\n",
        "\n",
        "        # identity shortcuts used directly when the input and output are of the same dimensions\n",
        "        if _project_shortcut or _strides != (1, 1):\n",
        "            # when the dimensions increase projection shortcut is used to match dimensions (done by 1×1 convolutions)\n",
        "            # when the shortcuts go across feature maps of two sizes, they are performed with a stride of 2\n",
        "            shortcut = layers.Conv2D(nb_channels_out, kernel_size=(1, 1), strides=_strides, padding='same')(shortcut)\n",
        "            shortcut = layers.BatchNormalization()(shortcut)\n",
        "\n",
        "        y = layers.add([shortcut, y])\n",
        "\n",
        "        # relu is performed right after each batch normalization,\n",
        "        # expect for the output of the block where relu is performed after the adding to the shortcut\n",
        "        y = layers.LeakyReLU()(y)\n",
        "\n",
        "        return y\n",
        "\n",
        "    # conv1\n",
        "    x = layers.Conv2D(64, kernel_size=(7, 7), strides=(2, 2), padding='same')(x)\n",
        "    x = add_common_layers(x)\n",
        "\n",
        "    # conv2\n",
        "    x = layers.MaxPool2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x)\n",
        "    for i in range(2):\n",
        "        project_shortcut = True if i == 0 else False\n",
        "        x = residual_block(x, 128, 256, _project_shortcut=project_shortcut)\n",
        "\n",
        "    # conv3\n",
        "    for i in range(1):\n",
        "        # down-sampling is performed by conv3_1, conv4_1, and conv5_1 with a stride of 2\n",
        "        strides = (2, 2) if i == 0 else (1, 1)\n",
        "        x = residual_block(x, 256, 512, _strides=strides)\n",
        "\n",
        "    # conv4\n",
        "    for i in range(1):\n",
        "        strides = (2, 2) if i == 0 else (1, 1)\n",
        "        x = residual_block(x, 512, 1024, _strides=strides)\n",
        "\n",
        "    # conv5\n",
        "    for i in range(1):\n",
        "        strides = (2, 2) if i == 0 else (1, 1)\n",
        "        x = residual_block(x, 1024, 2048, _strides=strides)\n",
        "\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dense(nb_classes, activation='softmax')(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "image_tensor = layers.Input(shape=input_shape)\n",
        "network_output = residual_network(image_tensor, nb_classes=nb_classes)\n",
        "  \n",
        "cnn = models.Model(inputs=[image_tensor], outputs=[network_output])\n",
        "print(cnn.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0j5oqgCYZs3T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**5. Training hyperparameters and functions.***"
      ]
    },
    {
      "metadata": {
        "id": "6QqBKbYBdwqi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_training(history, plot_val=False):\n",
        "  \"\"\"Plot the training and validation loss + accuracy\"\"\"\n",
        "  acc = history.history['acc']\n",
        "  # val_acc = history.history['val_acc']\n",
        "  loss = history.history['loss']\n",
        "  # val_loss = history.history['val_loss']\n",
        "  epochs = range(len(acc))\n",
        "\n",
        "  f = plt.figure()\n",
        "  plt.subplot(121)\n",
        "  plt.plot(epochs, acc, label='Train')\n",
        "  if plot_val:\n",
        "    plt.plot(epochs, val_acc, label='Val')\n",
        "  plt.title('Training and validation accuracy')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.subplot(122)\n",
        "  plt.plot(epochs, loss, label='Train')\n",
        "  if plot_val:\n",
        "    plt.plot(epochs, val_loss, label='Val')\n",
        "  plt.title('Training and validation loss')\n",
        "  plt.legend()\n",
        "  plt.savefig('acc_vs_epochs.png')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "filepath=\"/content/fietuned_ResNet50_model_weights.h5\"\n",
        "epochs = 10  # How many loops through the entire dataset\n",
        "num_train_images = len(x_train)\n",
        "lr = 1e-2\n",
        "adam = Adam(lr=lr)\n",
        "lr_reducer = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=np.sqrt(0.1),\n",
        "    cooldown=0,\n",
        "    patience=10,\n",
        "    min_lr=0.5e-6)\n",
        "early_stopper = EarlyStopping(\n",
        "    monitor='val_acc',\n",
        "    min_delta=0.0001,\n",
        "    patience=20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lcEbNW3bZy19",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**6. Train the fGRU.**"
      ]
    },
    {
      "metadata": {
        "id": "eSPIsviD7dE0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_checkpoint= ModelCheckpoint(\n",
        "    \"fgru_v1.h5\",\n",
        "    monitor=\"val_acc\",\n",
        "    save_best_only=True,\n",
        "    save_weights_only=True)\n",
        "callbacks=[lr_reducer, early_stopper, model_checkpoint]\n",
        "checkpoint = ModelCheckpoint(filepath, monitor=[\"acc\"], verbose=1, mode='max')\n",
        "fgru.compile(adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "fgru_history = fgru.fit_generator(\n",
        "  generator.flow(x_train, y_train, batch_size=batch_size),\n",
        "  samples_per_epoch=len(x_train),\n",
        "  nb_epoch=epochs,\n",
        "  validation_data=(x_test, y_test),\n",
        "  nb_val_samples=len(x_test),\n",
        "  shuffle=True,\n",
        "  callbacks=callbacks)\n",
        "plot_training(fgru_history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HKrUj_IaZ64a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**7. Train the hGRU.**"
      ]
    },
    {
      "metadata": {
        "id": "VjqmynmS7i3x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_checkpoint= ModelCheckpoint(\n",
        "    \"hgru_v1.h5\",\n",
        "    monitor=\"val_acc\",\n",
        "    save_best_only=True,\n",
        "    save_weights_only=True)\n",
        "callbacks=[lr_reducer, early_stopper, model_checkpoint]\n",
        "checkpoint = ModelCheckpoint(filepath, monitor=[\"acc\"], verbose=1, mode='max')\n",
        "hgru.compile(adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "hgru_history = hgru.fit_generator(\n",
        "  generator.flow(x_train, y_train, batch_size=batch_size),\n",
        "  samples_per_epoch=len(x_train),\n",
        "  nb_epoch=epochs,\n",
        "  validation_data=(x_test, y_test),\n",
        "  nb_val_samples=len(x_test),\n",
        "  shuffle=True,\n",
        "  callbacks=callbacks)\n",
        "plot_training(hgru_history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EFTYYLq2Z_43",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**8. Train a ResNet control.**"
      ]
    },
    {
      "metadata": {
        "id": "B7o8JNBO7YED",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_checkpoint= ModelCheckpoint(\n",
        "    \"cnn_v1.h5\",\n",
        "    monitor=\"val_acc\",\n",
        "    save_best_only=True,\n",
        "    save_weights_only=True)\n",
        "callbacks=[lr_reducer, early_stopper, model_checkpoint]\n",
        "checkpoint = ModelCheckpoint(filepath, monitor=[\"acc\"], verbose=1, mode='max')\n",
        "cnn.compile(adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "cnn_history = hgru.fit_generator(\n",
        "  generator.flow(x_train, y_train, batch_size=batch_size),\n",
        "  samples_per_epoch=len(x_train),\n",
        "  nb_epoch=epochs,\n",
        "  validation_data=(x_test, y_test),\n",
        "  nb_val_samples=len(x_test),\n",
        "  shuffle=True,\n",
        "  callbacks=callbacks)\n",
        "plot_training(cnn_history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0onz80BfaLVq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**9. Check adversarial tolerance.**"
      ]
    },
    {
      "metadata": {
        "id": "E7HzzlgVbZ7E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import foolbox\n",
        "\n",
        "\n",
        "keras.backend.set_learning_phase(0)\n",
        "\n",
        "\n",
        "cnn_fmodel = foolbox.models.KerasModel(cnn, bounds=(x_train.min(), x_train.max()))\n",
        "cnn_attack = foolbox.attacks.FGSM(cnn_fmodel)\n",
        "cnn_adversarial = cnn_attack(x_test[0], y_test[0])\n",
        "print cnn_adversarial\n",
        "# if the attack fails, adversarial will be None and a warning will be printed\n",
        "\n",
        "fgru_fmodel = foolbox.models.KerasModel(fgru, bounds=(x_train.min(), x_train.max()))\n",
        "fgru_attack = foolbox.attacks.FGSM(fgru_fmodel)\n",
        "fgru_adversarial = fgru_attack(x_test[0], y_test[0])\n",
        "print fgru_adversarial\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FtoD7oIXaO23",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**10. Check noise tolerance.**\n"
      ]
    },
    {
      "metadata": {
        "id": "LwifGjtyaOXL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sklearn.metrics as metrics\n",
        "\n",
        "\n",
        "# Compare performance on noisy versus non-noisy images\n",
        "kernel = 0.1\n",
        "noise_x_test = x_test + np.random.random(size=x_test.shape) * kernel\n",
        "noise_x_test = np.minimum(np.maximum(x_test.max(), noise_x_test), noise_x_test)\n",
        "\n",
        "# Regular\n",
        "y_preds = cnn.predict(x_test)\n",
        "y_pred = np.argmax(y_preds, axis=-1)\n",
        "arg_y = np.argmax(y_test, axis=-1)\n",
        "cnn_accuracy = metrics.accuracy_score(arg_y, y_pred) * 100\n",
        "y_preds = fgru.predict(x_test)\n",
        "y_pred = np.argmax(y_preds, axis=-1)\n",
        "fgru_accuracy = metrics.accuracy_score(arg_y, y_pred) * 100\n",
        "\n",
        "# Noisy\n",
        "noise_y_preds = cnn.predict(noise_x_test)\n",
        "noise_y_pred = np.argmax(noise_y_preds, axis=-1)\n",
        "noise_cnn_accuracy = metrics.accuracy_score(arg_y, noise_y_pred) * 100\n",
        "noise_y_preds = fgru.predict(noise_x_test)\n",
        "noise_y_pred = np.argmax(noise_y_preds, axis=-1)\n",
        "noise_fgru_accuracy = metrics.accuracy_score(arg_y, noise_y_pred) * 100\n",
        "\n",
        "print 'CNN goes from %s%% to %s%% with %s uniform noise' % (cnn_accuracy, noise_cnn_accuracy, kernel)\n",
        "print 'fGRU goes from %s%% to %s%% with %s uniform noise' % (fgru_accuracy, noise_fgru_accuracy, kernel)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}